"""demo_validation.py â€” Script de validation pour la prÃ©sentation.

Charge les rÃ©sultats d'une analyse existante et affiche un rÃ©sumÃ©
structurÃ© qui peut servir de support visuel pendant la dÃ©mo.

Usage:
    python demo_validation.py outputs/1772018049-nihms-100109-8037d1af
"""
from __future__ import annotations

import json
import sys
from pathlib import Path


def load(path: Path) -> dict | list | None:
    if not path.exists():
        return None
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def fmt_score(val: float | None) -> str:
    if val is None:
        return "â€”"
    pct = int(val * 100)
    bar = "â–ˆ" * (pct // 5) + "â–‘" * (20 - pct // 5)
    return f"{val:.2f} [{bar}]"


def main(output_dir: str) -> None:
    d = Path(output_dir)
    if not d.exists():
        print(f"Dossier introuvable : {d}")
        sys.exit(1)

    meta = load(d / "00_metadata.json")
    effects = load(d / "04_effects.json")
    quality = load(d / "05_quality_quick.json")
    credibility = load(d / "06_external_credibility.json")
    article_eval = load(d / "08_article_evaluation.json")
    reliability = load(d / "07_summary_score.json")

    print("=" * 70)
    print("  IMPACT â€” RÃ©sumÃ© de l'analyse")
    print("=" * 70)

    # â”€â”€ MÃ©tadonnÃ©es â”€â”€
    print("\nðŸ“„ MÃ‰TADONNÃ‰ES")
    print("-" * 50)
    if article_eval:
        print(f"  Titre    : {article_eval.get('title', '?')}")
        authors = article_eval.get("authors_extracted", [])
        names = [a.get("full_name", "?") if isinstance(a, dict) else str(a) for a in authors[:4]]
        print(f"  Auteurs  : {', '.join(names)}")
        journal = article_eval.get("journal_extracted", {})
        print(f"  Revue    : {journal.get('name', 'â€”') if isinstance(journal, dict) else 'â€”'}")
        print(f"  AnnÃ©e    : {article_eval.get('year', 'â€”')}")
        print(f"  DOI      : {article_eval.get('doi', 'â€”')}")
        print(f"  Type     : {article_eval.get('document_type', 'journal_article')}")
        org = article_eval.get("organization")
        if org:
            print(f"  Org.     : {org}")
    elif meta:
        print(f"  Titre    : {meta.get('title', '?')}")
        print(f"  Auteurs  : {', '.join(meta.get('authors', ['?']))}")

    # â”€â”€ Score Article â”€â”€
    print("\nðŸ“Š SCORE DE RENOMMÃ‰E")
    print("-" * 50)
    if article_eval and "scores" in article_eval:
        scores = article_eval["scores"]
        g = scores.get("global", {})
        global_val = g.get("value", 0) if isinstance(g, dict) else 0
        print(f"  Global           : {fmt_score(global_val)}")
        for dim in ["article", "journal", "author", "field_norm", "network"]:
            sub = scores.get(dim, {})
            val = sub.get("score", 0) if isinstance(sub, dict) else 0
            label = {
                "article": "Article (citations)",
                "journal": "Revue (SCImago)",
                "author": "Auteur (h-index)",
                "field_norm": "Champ (normalisation)",
                "network": "RÃ©seau (institutions)",
            }.get(dim, dim)
            print(f"  {label:<22}: {fmt_score(val)}")

        # Details
        art = scores.get("article", {})
        if isinstance(art, dict) and "raw_citations" in art:
            print(f"\n  â†’ Citations : {art['raw_citations']}")
        auth = scores.get("author", {})
        if isinstance(auth, dict) and "aggregated_h_index" in auth:
            print(f"  â†’ h-index max : {auth['aggregated_h_index']}")
        j = scores.get("journal", {})
        if isinstance(j, dict) and "scimago_quartile" in j:
            print(f"  â†’ SCImago : {j['scimago_quartile']} (SJR={j.get('scimago_sjr', '?')})")
    else:
        print("  [DonnÃ©es non disponibles]")

    # â”€â”€ Effets â”€â”€
    print("\nðŸ”¬ EFFETS EXTRAITS")
    print("-" * 50)
    if effects and isinstance(effects, dict):
        eff_list = effects.get("effects", [])
        study_fx = [e for e in eff_list if isinstance(e, dict) and e.get("effect_scope") == "study_effect"]
        cited_fx = [e for e in eff_list if isinstance(e, dict) and e.get("effect_scope") == "cited_effect"]
        model_fx = [e for e in eff_list if isinstance(e, dict) and e.get("effect_scope") == "model_stat"]
        print(f"  Total         : {len(eff_list)}")
        print(f"  Ã‰tude         : {len(study_fx)}")
        print(f"  CitÃ©s         : {len(cited_fx)}")
        print(f"  Stats modÃ¨le  : {len(model_fx)}")

        if study_fx:
            print(f"\n  {'Groupe':<12} {'Outcome':<25} {'Type':<5} {'Valeur':<8} {'Source':<20} {'Page':<5}")
            print(f"  {'â”€'*12} {'â”€'*25} {'â”€'*5} {'â”€'*8} {'â”€'*20} {'â”€'*5}")
            for e in study_fx[:10]:
                spec = e.get("result_spec", {}) or {}
                group = str(e.get("grouping_label") or spec.get("groups") or "â€”")[:12]
                outcome = str(spec.get("outcome") or "â€”")[:25]
                et = str(e.get("effect_type", "?"))
                val = e.get("value")
                val_str = f"{val:+.2f}" if val is not None else "â€”"
                source = str(e.get("source_kind", "â€”"))[:20]
                page = str(e.get("source_page", "â€”"))
                print(f"  {group:<12} {outcome:<25} {et:<5} {val_str:<8} {source:<20} {page:<5}")

            # Quotes
            print(f"\n  ðŸ“ Passages source :")
            for i, e in enumerate(study_fx[:3], 1):
                quote = e.get("quote", "")
                if quote:
                    print(f"  [{i}] \"{quote[:120]}{'â€¦' if len(quote) > 120 else ''}\"")
    else:
        print("  [Aucun effet extrait]")

    # â”€â”€ QualitÃ© mÃ©thodologique â”€â”€
    print("\nðŸ§ª QUALITÃ‰ MÃ‰THODOLOGIQUE")
    print("-" * 50)
    if quality and isinstance(quality, dict):
        design = quality.get("study_design", "unknown")
        design_labels = {
            "RCT": "Essai contrÃ´lÃ© randomisÃ©",
            "quasi_experimental": "Quasi-expÃ©rimental",
            "observational_longitudinal": "Longitudinal",
            "observational_cross_sectional": "Transversal",
            "meta_analysis": "MÃ©ta-analyse",
            "case_study": "Ã‰tude de cas",
        }
        print(f"  Design         : {design_labels.get(design, design)}")
        print(f"  Justification  : {(quality.get('study_design_justification') or 'â€”')[:100]}")
        n = quality.get("sample_size_n")
        print(f"  N              : {n if n else 'â€”'}")
        print(f"  Score interne  : {fmt_score(quality.get('internal_quality_score', 0))}")

        flags = [
            ("Randomisation", "randomization"),
            ("Groupe contrÃ´le", "control_group"),
            ("Taille Ã©chant.", "sample_size_reported"),
            ("Attrition", "attrition_reported"),
            ("Aveugle", "blinding_reported"),
        ]
        print(f"\n  {'Indicateur':<18} {'Statut':<10} {'Justification':<50}")
        print(f"  {'â”€'*18} {'â”€'*10} {'â”€'*50}")
        for label, key in flags:
            val = quality.get(key, "unclear")
            icon = "âœ… oui" if val == "yes" else ("âŒ non" if val == "no" else "â” ??")
            justif = (quality.get(f"{key}_justification") or "â€”")[:50]
            print(f"  {label:<18} {icon:<10} {justif:<50}")
    else:
        print("  [DonnÃ©es non disponibles]")

    # â”€â”€ CrÃ©dibilitÃ© externe â”€â”€
    print("\nðŸŒ CRÃ‰DIBILITÃ‰ EXTERNE")
    print("-" * 50)
    if credibility and isinstance(credibility, dict):
        print(f"  Score          : {fmt_score(credibility.get('external_score', 0))}")
        print(f"  Niveau         : {credibility.get('credibility_level', 'â€”')}")
        print(f"  Venue          : {credibility.get('venue', 'â€”')}")
        print(f"  Publisher      : {credibility.get('publisher', 'â€”')}")
        cit = credibility.get("citation_count")
        print(f"  Citations      : {cit if cit is not None else 'â€”'}")
    else:
        print("  [DonnÃ©es non disponibles]")

    # â”€â”€ FiabilitÃ© globale â”€â”€
    print("\nðŸ“‹ SCORE GLOBAL")
    print("-" * 50)
    if reliability and isinstance(reliability, dict):
        print(f"  Score global   : {fmt_score(reliability.get('global_score', 0))}")
        print(f"  Conclusion     : {reliability.get('conclusion', 'â€”')}")
        items = reliability.get("items", [])
        if items:
            print(f"  Effets notÃ©s   : {len(items)}")
    else:
        print("  [DonnÃ©es non disponibles]")

    # â”€â”€ CoÃ»ts estimÃ©s â”€â”€
    print("\nðŸ’° ESTIMATION DE COÃ›T")
    print("-" * 50)
    if article_eval:
        notes = article_eval.get("notes", [])
        n_steps = len([n for n in notes if "=" in n])
        print(f"  Steps exÃ©cutÃ©s : {n_steps}")
    print(f"  ModÃ¨le         : gpt-4.1-mini")
    print(f"  Calls API est. : ~8-15 par PDF")
    print(f"  CoÃ»t estimÃ©    : ~0.02â€“0.08 â‚¬ / PDF")

    print("\n" + "=" * 70)
    print("  Fin du rÃ©sumÃ©")
    print("=" * 70)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        # Default to latest analysis
        outputs = Path("outputs")
        dirs = sorted(
            [d for d in outputs.iterdir() if d.is_dir() and not d.name.startswith("_")],
            key=lambda x: x.name,
            reverse=True,
        )
        if dirs:
            print(f"[Auto-sÃ©lection du dernier dossier : {dirs[0].name}]\n")
            main(str(dirs[0]))
        else:
            print("Usage: python demo_validation.py <dossier_analyse>")
            sys.exit(1)
    else:
        main(sys.argv[1])
